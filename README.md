# unet-segmentation
Image segmentation with U-Net on an autonomous driving dataset.

## Objective
Semantic image segmentation, in which images are classified on a per-pixel basis, has become a primary focus in computer vision and machine learning. Semantic segmentation is in high demand across a wide variety of fields such as radiological imaging, in which different body parts may be labeled or benign tissue distinguished from malignant, and autonomous driving, in which the surrounding environment is classified on a per-pixel basis. 

This repository implements a U-Net trained for image segmentation on an autonomous driving image dataset. 

## Dataset
Training is performed on an image dataset available from [CARLA](https://carla.org/). The dataset consists of 1060 RGBA images, and for each image, a mask encoding the ground truth segmentation of the corresponding image. Images appear to be computer-generated by a 3D renderer, and visually approximate a view of a typical surrounding environment that would be captured by a forward-facing car-mounted camera. This dataset contains 23 different object classes. 

## U-Net
U-Net is a neural network architecture frequently used in image segmentation for its precision and speed. U-Net consists of a contracting path in which successive convolutions downsample, and an expanding path in which transpose convolutions upsample from a combination of a prior expanding path layer and a corresponding contracting path layer via a skip connection. These skip connections preserve information that would otherwise be lost were only downsampling performed.

### U-Net Implementation
In view of hardware limitations and the expense of training a full U-Net, this repository instead implements a "half-U-Net" consisting of the same number of layers/paths but with half the number of filters in each layer/path. Despite this simplification, high performance on the training set is observed with relatively thorough training.

## Script Use
`script.py` implements various functions for visualizing training data and U-Net predictions, and training/inferencing a half-U-Net model:
- `show_ground_truth()` displays an unprocessed training image/mask pair
- `train_model()` constructs and trains a half-U-Net on a training set. RGBA images are expected in the `./data/images` folder, and masks are expected in the `./data/masks` folder. By default, input images are downsampled to 96x128, which is the size of predicted masks output by the U-Net model. 
- `show_processed_images()` displays a processed training image/mask pair
- `show_predictions()` displays an image triplet consisting of a processed training image, a processed corresponding mask, and a predicted mask output by the U-Net model for the training image

### Loading a Pre-Trained U-Net
By default, `script.py` loads a pre-trained half-U-Net trained on the aforementioned CARLA dataset, and outputs a predicted mask for an example training image. A model is expected in the `./unet_model` folder. Due to the size of the model's weights (found in the `./unet_model/variables` folder), weights are split into 10 MB segments, and should be combined into a single file named `variables.data-00000-of-00001` and placed in the `./unet_model/variables` folder. The application 7Zip may be used to perform this operation, for example.

## Results
As seen in the graph below that plots accuracy versus training epoch, high performance is observed on the training set after training for 40 epochs:

![accuracy_v_epoch](https://user-images.githubusercontent.com/491395/166836206-37e03834-c03d-4f0e-8983-8dc5e9f6ce18.png)

## Limitations and Future Development
While high performance is observed on the training set after substantial training, higher performance could likely be obtained with further training, and/or by building a full U-Net with the standard number of filters. 

For the application of autonomous driving, high performance on unseen images would be of paramount performance. To this end, expanding the training set with non-rendered images representing a wider variety of environments, lighting conditions, and weather conditions would likely be prudent.
